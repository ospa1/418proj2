{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.figure_factory._county_choropleth import create_choropleth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = pd.read_csv(\"merged_train.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 1 \n",
    "# Partition dataset into training, validation sets using holdout method 75/25 split\n",
    "\n",
    "# Xvariables has all the names except for State, County, FIPS, Party, Democratic, Republican\n",
    "# Yvariables has Party, Democratic, Republican\n",
    "Yvariables = ['Party','Democratic','Republican']\n",
    "Xvariables = ['Total Population', 'Percent White, not Hispanic or Latino', 'Percent Black, not Hispanic or Latino','Percent Hispanic or Latino','Percent Foreign Born','Percent Female','Percent Age 29 and Under','Percent Age 65 and Older','Median Household Income','Percent Unemployed','Percent Less than High School Degree','Percent Less than Bachelor\\'s Degree','Percent Rural']\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(data[Xvariables], data[Yvariables], test_size = 0.25, random_state = 1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 2\n",
    "# standardize the training and validation sets by using X_train as the scalar and applying to the training and validation sets\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train) # find the mean and standard diviation for the columns in X_train\n",
    "x_train_scaled = scaler.transform(X_train) # scales X_train using the results from fit method\n",
    "x_val_scaled = scaler.transform(X_val)  # scales X_val using the results from fit method\n",
    "x_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 3\n",
    "# predict democratic votes using 2 predictors\n",
    "model = linear_model.LinearRegression()\n",
    "fitted_model = model.fit(X=X_train[['Percent Foreign Born', 'Median Household Income']],y=Y_train['Democratic'])\n",
    "predicted = fitted_model.predict(X_train[['Percent Foreign Born','Median Household Income']])\n",
    "print (predicted)\n",
    "\n",
    "# Evaluate linear regression model using 2 predictors on democratic votes\n",
    "X_train_dummy = pd.get_dummies(X_train, drop_first = True)\n",
    "X_val_dummy = pd.get_dummies(X_val, drop_first = True)\n",
    "model = linear_model.LinearRegression().fit(X = X_train_dummy[['Percent Foreign Born', 'Median Household Income']], y = Y_train['Democratic'])\n",
    "score_val = model.score(X = X_val_dummy[['Percent Foreign Born', 'Median Household Income']], y = Y_val['Democratic']) # R squared (validation)\n",
    "print(score_val)\n",
    "\n",
    "# Evaluate LASSO regression model on democratic votes\n",
    "model = linear_model.Lasso(alpha = 1).fit(X = X_train_dummy, y = Y_train['Democratic'])\n",
    "score_val = model.score(X = X_val_dummy, y = Y_val['Democratic']) # R squared (validation)\n",
    "print(score_val)\n",
    "\n",
    "# predict republican votes using 2 predictors\n",
    "model = linear_model.LinearRegression()\n",
    "fitted_model = model.fit(X=X_train[['Percent Foreign Born', 'Median Household Income']],y=Y_train['Republican'])\n",
    "predicted = fitted_model.predict(X_train[['Percent Foreign Born', 'Median Household Income']])\n",
    "print (predicted)\n",
    "\n",
    "# Evaluate linear regression model using 2 predictors on republican votes\n",
    "X_train_dummy = pd.get_dummies(X_train, drop_first = True)\n",
    "X_val_dummy = pd.get_dummies(X_val, drop_first = True)\n",
    "model = linear_model.LinearRegression().fit(X = X_train_dummy[['Percent Foreign Born', 'Median Household Income']], y = Y_train['Republican'])\n",
    "score_val = model.score(X = X_val_dummy[['Percent Foreign Born', 'Median Household Income']], y = Y_val['Republican']) # R squared (validation)\n",
    "print(score_val)\n",
    "\n",
    "# Evaluate LASSO regression model on republican votes\n",
    "model = linear_model.Lasso(alpha = 1).fit(X = X_train_dummy, y = Y_train['Republican'])\n",
    "score_val = model.score(X = X_val_dummy, y = Y_val['Republican']) # R squared (validation)\n",
    "print(score_val)\n",
    "\n",
    "# it seems like lasso regression is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 4\n",
    "# Partitioning the data into training and validation using only variables that are stronger predictors\n",
    "X = ['Percent White, not Hispanic or Latino', 'Percent Black, not Hispanic or Latino', 'Percent Hispanic or Latino', 'Percent Age 29 and Under', 'Percent Age 65 and Older', 'Percent Less than High School Degree', 'Percent Less than Bachelor\\'s Degree', 'Percent Rural']\n",
    "Y = ['Party']\n",
    "X_train_class, X_val_class, Y_train_class, Y_val_class = train_test_split(data[X], data[Y], test_size=0.25, random_state=0)\n",
    "scaler.fit(X_train_class)\n",
    "x_train_scaled_class = scaler.transform(X_train_class)\n",
    "x_val_scaled_class = scaler.transform(X_val_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 4 - continuation\n",
    "# Build a classification model to classify each county as Democratic or Republican\n",
    "# kNN classifier\n",
    "classifier = KNeighborsClassifier(n_neighbors=5, weights='uniform')  \n",
    "classifier.fit(x_train_scaled_class, Y_train_class.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = classifier.predict(x_val_scaled_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = metrics.confusion_matrix(Y_val_class, y_pred_class)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(conf_matrix, annot = True, fmt = \".3f\", square = True, cmap = plt.cm.Blues)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion matrix')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy_score(Y_val_class, y_pred_class)\n",
    "error = 1 - accuracy\n",
    "precision = metrics.precision_score(Y_val_class, y_pred_class, average=None)\n",
    "recall = metrics.recall_score(Y_val_class, y_pred_class, average=None)\n",
    "F1_score = metrics.f1_score(Y_val_class, y_pred_class, average=None)\n",
    "print(\"Results of kNN classifier\\n\")\n",
    "print(\"Accuracy: \" + str(accuracy) + \"\\n\" + \"Error: \" + str(error) + \"\\n\" + \"Precision: \" + str(precision) + \"\\n\" + \"Recall: \" + str(recall) + \"\\n\" + \"F1 score: \" + str(F1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 4 - continuation\n",
    "# SVM Classifier\n",
    "classifier = SVC(kernel='rbf')\n",
    "classifier.fit(x_train_scaled_class, Y_train_class.values.ravel())\n",
    "\n",
    "svm_class = classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = classifier.predict(x_val_scaled_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = metrics.confusion_matrix(Y_val_class, y_pred_class)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(conf_matrix, annot = True, fmt = \".3f\", square = True, cmap = plt.cm.Blues)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion matrix')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy_score(Y_val_class, y_pred_class)\n",
    "error = 1 - accuracy\n",
    "precision = metrics.precision_score(Y_val_class, y_pred_class, average=None)\n",
    "recall = metrics.recall_score(Y_val_class, y_pred_class, average=None)\n",
    "F1_score = metrics.f1_score(Y_val_class, y_pred_class, average=None)\n",
    "print(\"Results of SVM classifier\\n\")\n",
    "print(\"Accuracy: \" + str(accuracy) + \"\\n\" + \"Error: \" + str(error) + \"\\n\" + \"Precision: \" + str(precision) + \"\\n\" + \"Recall: \" + str(recall) + \"\\n\" + \"F1 score: \" + str(F1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 4\n",
    "# Decision trees\n",
    "classifier = DecisionTreeClassifier(criterion=\"entropy\", random_state=0)\n",
    "classifier.fit(x_train_scaled_class, Y_train_class.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of nodes in the decision tree\n",
    "print(\"Number of nodes in the decision tree: \" + str(len(classifier.tree_.__getstate__()['nodes'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = classifier.predict(x_val_scaled_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = metrics.confusion_matrix(Y_val_class, y_pred_class)\n",
    "sns.heatmap(conf_matrix, annot = True, fmt = \".3f\", square = True, cmap = plt.cm.Blues)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion matrix')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy_score(Y_val_class, y_pred_class)\n",
    "error = 1 - accuracy\n",
    "precision = metrics.precision_score(Y_val_class, y_pred_class, average=None)\n",
    "recall = metrics.recall_score(Y_val_class, y_pred_class, average=None)\n",
    "F1_score = metrics.f1_score(Y_val_class, y_pred_class, average=None)\n",
    "print(\"Results of Decision trees classifier\\n\")\n",
    "print(\"Accuracy: \" + str(accuracy) + \"\\n\" + \"Error: \" + str(error) + \"\\n\" + \"Precision: \" + str(precision) + \"\\n\" + \"Recall: \" + str(recall) + \"\\n\" + \"F1 score: \" + str(F1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 4\n",
    "# Naives Bayes Classifier\n",
    "classifier = GaussianNB()  \n",
    "classifier.fit(x_train_scaled_class, Y_train_class.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = classifier.predict(x_val_scaled_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = metrics.confusion_matrix(Y_val_class, y_pred_class)\n",
    "sns.heatmap(conf_matrix, annot = True, fmt = \".3f\", square = True, cmap = plt.cm.Blues)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion matrix')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy_score(Y_val_class, y_pred_class)\n",
    "error = 1 - accuracy\n",
    "precision = metrics.precision_score(Y_val_class, y_pred_class, average=None)\n",
    "recall = metrics.recall_score(Y_val_class, y_pred_class, average=None)\n",
    "F1_score = metrics.f1_score(Y_val_class, y_pred_class, average=None)\n",
    "print(\"Results of Naive Bayes classifier\\n\")\n",
    "print(\"Accuracy: \" + str(accuracy) + \"\\n\" + \"Error: \" + str(error) + \"\\n\" + \"Precision: \" + str(precision) + \"\\n\" + \"Recall: \" + str(recall) + \"\\n\" + \"F1 score: \" + str(F1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Classifier has the highest accuracy among all the 4 classification methods for the selected predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5 Start Kmeans 2 clusters all vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ['Total Population', 'Percent White, not Hispanic or Latino', 'Percent Black, not Hispanic or Latino','Percent Hispanic or Latino','Percent Foreign Born','Percent Female','Percent Age 29 and Under','Percent Age 65 and Older','Median Household Income','Percent Unemployed','Percent Less than High School Degree','Percent Less than Bachelor\\'s Degree','Percent Rural']\n",
    "Y = ['Party']\n",
    "\n",
    "train_class = data[X]\n",
    "lables = data[Y].to_numpy().reshape(-1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_class)\n",
    "train_scaled_class = scaler.transform(train_class)\n",
    "\n",
    "clustering = KMeans(n_clusters=2, init='random', random_state=0).fit(train_scaled_class)\n",
    "clusters = clustering.labels_\n",
    "\n",
    "cont_matrix = metrics.cluster.contingency_matrix(lables, clusters)\n",
    "sns.heatmap(cont_matrix, annot = True, fmt = \".3f\", square = True, cmap = plt.cm.Blues)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Contingency matrix')\n",
    "plt.tight_layout()\n",
    "\n",
    "adjusted_rand_index = metrics.adjusted_rand_score(lables, clusters)\n",
    "silhouette_coefficient = np.average(metrics.silhouette_samples(train_scaled_class, clusters, metric='euclidean'))\n",
    "print([adjusted_rand_index, silhouette_coefficient])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB Scan eps = 2 min_samples = 5 \n",
    "clustering = DBSCAN(eps=2,min_samples=5,metric='euclidean').fit(train_scaled_class)\n",
    "clusters = clustering.labels_\n",
    "\n",
    "cont_matrix = metrics.cluster.contingency_matrix(lables, clusters)\n",
    "sns.heatmap(cont_matrix, annot = True, fmt = \".3f\", square = True, cmap = plt.cm.Blues)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Contingency matrix')\n",
    "plt.tight_layout()\n",
    "\n",
    "adjusted_rand_index = metrics.adjusted_rand_score(lables, clusters)\n",
    "silhouette_coefficient = np.average(metrics.silhouette_samples(train_scaled_class, clusters, metric='euclidean'))\n",
    "print([adjusted_rand_index, silhouette_coefficient])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percent White, not Hispanic or Latino', 'Percent Black, not Hispanic or Latino','Percent Hispanic or Latino','Percent Foreign Born'\n",
    "# Kmeans init = 'k-means++' n init = 15\n",
    "X = ['Percent White, not Hispanic or Latino', 'Percent Black, not Hispanic or Latino','Percent Hispanic or Latino','Percent Foreign Born']\n",
    "Y = ['Party']\n",
    "\n",
    "\n",
    "train_class = data[X]\n",
    "lables = data[Y].to_numpy().reshape(-1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_class)\n",
    "train_scaled_class = scaler.transform(train_class)\n",
    "\n",
    "clustering = KMeans(n_clusters=2, n_init= 15, init='k-means++', random_state=0).fit(train_scaled_class)\n",
    "clusters = clustering.labels_\n",
    "\n",
    "cont_matrix = metrics.cluster.contingency_matrix(lables, clusters)\n",
    "sns.heatmap(cont_matrix, annot = True, fmt = \".3f\", square = True, cmap = plt.cm.Blues)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Contingency matrix')\n",
    "plt.tight_layout()\n",
    "\n",
    "adjusted_rand_index = metrics.adjusted_rand_score(lables, clusters)\n",
    "silhouette_coefficient = np.average(metrics.silhouette_samples(train_scaled_class, clusters, metric='euclidean'))\n",
    "print([adjusted_rand_index, silhouette_coefficient])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB Scan eps = 2 min samples = 8\n",
    "clustering = DBSCAN(eps=2,min_samples=8,metric='euclidean').fit(train_scaled_class)\n",
    "clusters = clustering.labels_\n",
    "\n",
    "cont_matrix = metrics.cluster.contingency_matrix(lables, clusters)\n",
    "sns.heatmap(cont_matrix, annot = True, fmt = \".3f\", square = True, cmap = plt.cm.Blues)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Contingency matrix')\n",
    "plt.tight_layout()\n",
    "\n",
    "adjusted_rand_index = metrics.adjusted_rand_score(lables, clusters)\n",
    "silhouette_coefficient = np.average(metrics.silhouette_samples(train_scaled_class, clusters, metric='euclidean'))\n",
    "print([adjusted_rand_index, silhouette_coefficient])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Total Population', 'Percent White, not Hispanic or Latino', 'Percent Black, not Hispanic or Latino','Percent Hispanic or Latino','Percent Foreign Born','Percent Female','Percent Age 29 and Under','Percent Age 65 and Older','Median Household Income','Percent Unemployed','Percent Less than High School Degree','Percent Less than Bachelor\\'s Degree','Percent Rural'\n",
    "# Kmeans init = 'k-means++' n init = 15\n",
    "X = ['Total Population', 'Percent White, not Hispanic or Latino', 'Percent Black, not Hispanic or Latino','Percent Hispanic or Latino','Percent Foreign Born','Percent Female','Percent Age 29 and Under','Percent Age 65 and Older','Median Household Income','Percent Unemployed','Percent Less than High School Degree','Percent Less than Bachelor\\'s Degree','Percent Rural']\n",
    "\n",
    "train_class = data[X]\n",
    "lables = data[Y].to_numpy().reshape(-1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_class)\n",
    "train_scaled_class = scaler.transform(train_class)\n",
    "\n",
    "clustering = KMeans(n_clusters=2, n_init= 15, init='k-means++', random_state=0).fit(train_scaled_class)\n",
    "clusters = clustering.labels_\n",
    "\n",
    "cont_matrix = metrics.cluster.contingency_matrix(lables, clusters)\n",
    "sns.heatmap(cont_matrix, annot = True, fmt = \".3f\", square = True, cmap = plt.cm.Blues)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Contingency matrix')\n",
    "plt.tight_layout()\n",
    "\n",
    "adjusted_rand_index = metrics.adjusted_rand_score(lables, clusters)\n",
    "silhouette_coefficient = np.average(metrics.silhouette_samples(train_scaled_class, clusters, metric='euclidean'))\n",
    "print([adjusted_rand_index, silhouette_coefficient])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB Scan eps =2 min samples =8\n",
    "clustering = DBSCAN(eps=2,min_samples=8,metric='euclidean').fit(train_scaled_class)\n",
    "clusters = clustering.labels_\n",
    "\n",
    "cont_matrix = metrics.cluster.contingency_matrix(lables, clusters)\n",
    "sns.heatmap(cont_matrix, annot = True, fmt = \".3f\", square = True, cmap = plt.cm.Blues)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Contingency matrix')\n",
    "plt.tight_layout()\n",
    "\n",
    "adjusted_rand_index = metrics.adjusted_rand_score(lables, clusters)\n",
    "silhouette_coefficient = np.average(metrics.silhouette_samples(train_scaled_class, clusters, metric='euclidean'))\n",
    "print([adjusted_rand_index, silhouette_coefficient])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 6\n",
    "X = ['Percent White, not Hispanic or Latino', 'Percent Black, not Hispanic or Latino', 'Percent Hispanic or Latino', 'Percent Age 29 and Under', 'Percent Age 65 and Older', 'Percent Less than High School Degree', 'Percent Less than Bachelor\\'s Degree', 'Percent Rural']\n",
    "\n",
    "_data = data[X]\n",
    "_fips = data[['FIPS']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(_data)\n",
    "scaled_data = scaler.transform(_data)\n",
    "\n",
    "party = svm_class.predict(scaled_data).tolist()\n",
    "fips = _fips.values\n",
    "\n",
    "colorscale = [\"#ff0000\",\"#0015bc\"]\n",
    "fig = create_choropleth(fips=fips, values=party, colorscale=colorscale, county_outline={'color': 'rgb(105,105,105)', 'width': 0.25}, state_outline={'color': 'rgb(192,192,192)','width': 1})\n",
    "fig.layout.template = None\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 7\n",
    "test_data = pd.read_csv(\"demographics_test.csv\")\n",
    "\n",
    "scaler.fit(X_train)\n",
    "x_test_scaled = scaler.transform(test_data[Xvariables])\n",
    "\n",
    "model = linear_model.Lasso(alpha=1).fit(X=X_train_dummy, y=Y_train['Democratic'])\n",
    "dem_predicted = model.predict(x_test_scaled)\n",
    "dem_predicted = dem_predicted.astype(int)\n",
    "\n",
    "model = linear_model.Lasso(alpha=1).fit(X=X_train_dummy, y=Y_train['Republican'])\n",
    "rep_predicted = model.predict(x_test_scaled)\n",
    "rep_predicted = rep_predicted.astype(int)\n",
    "\n",
    "class_test = test_data.iloc[:, [4, 5, 6, 9, 10, 13, 14, 15]]\n",
    "scaler.fit(class_test)\n",
    "class_test_scaled = scaler.transform(class_test)\n",
    "\n",
    "test_pred = svm_class.predict(class_test_scaled)\n",
    "\n",
    "a = np.array([test_data['State'], test_data['County'], dem_predicted, rep_predicted, test_pred])\n",
    "a = np.column_stack(a)\n",
    "np.savetxt(\"output.csv\", a, delimiter=',', header=\"State,County,Democratic,Republican,Party\", fmt=\"%s\", comments=\"\")\n",
    "\n",
    "# the output of this task is stored in the file 'output.csv'"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
